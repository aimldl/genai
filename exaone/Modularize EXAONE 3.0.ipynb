{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c972b40-599a-4846-8d79-19097ac22ebf",
   "metadata": {},
   "source": [
    "# Modularize EXAONE 3.0 to Test it Quickly\n",
    "- Created: 2024-12-02 (Mon)\n",
    "- Updated: 2024-12-02 (Mon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1e7092-7906-488d-9b07-d2e833b85770",
   "metadata": {},
   "source": [
    "Relevant link: https://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaec9a9-eaa4-49ca-bddf-93e8b9f695bc",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to make the code more compact and readable than [Getting Started with EXAONE 3.0.ipynb](Getting Started with EXAONE 3.0.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9eeeff-0cf1-4c4a-9540-c2f869f214b3",
   "metadata": {},
   "source": [
    "## Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a917f45-f7a4-4539-9dc8-ce621b2ab7db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ModuleNotFoundError: No mo|dule named 'torch'\n",
    "!pip install --quiet torch torchvision torchaudio transformers huggingface_hub accelerate>=0.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4280895f-860b-4624-98cd-60b7a463cabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart runtime\n",
    "#   Otherwise, the ImportError won't go away.\n",
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970f07ad-7a83-4eca-922a-ce0585eaba29",
   "metadata": {},
   "source": [
    "## Log into Hugging Face Hub\n",
    "The Hugging Face token is created and save in `token-to-access-exaone-since-2024-q4.txt`.\n",
    "\n",
    "Open a new terminal (File > New > Terminal) and run:\n",
    "```bash\n",
    "$ huggingface-cli login\n",
    "# Paste your access token when prompted.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c234a0-e76f-4775-9714-6f48d1009c9c",
   "metadata": {},
   "source": [
    "## Modularize the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db5cf3b0-701e-4e1a-aaa7-208957d59c71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def print_now():\n",
    "  \"\"\"\n",
    "  Prints the current date and time in the format YYYY-MM-DD HH:MM:SS.\n",
    "    Args: None\n",
    "    Returns: None (prints the decoded output)    \n",
    "  # Example usage\n",
    "    print_now()\n",
    "  # 2024-12-02 06:59:49\n",
    "  \"\"\"\n",
    "  now = datetime.datetime.now()\n",
    "  formatted_now = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "  print(formatted_now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcd76c5f-800a-4212-9847-c308d022929c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-02 06:59:49\n"
     ]
    }
   ],
   "source": [
    "print_now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbb99b44-1918-443d-845e-d593e6a069b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def load_exaone():\n",
    "  \"\"\"\n",
    "  Loads and returns EXAONE's model and tokenizer.\n",
    "    Args: None\n",
    "    Returns: model, tokenizer\n",
    "  # Example usage\n",
    "    model, tokenizer = load_exaone() \n",
    "  \"\"\"\n",
    "\n",
    "  model = AutoModelForCausalLM.from_pretrained(\n",
    "      \"LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct\",\n",
    "      torch_dtype=torch.bfloat16,\n",
    "      trust_remote_code=True,\n",
    "      device_map=\"auto\"\n",
    "  )\n",
    "  tokenizer = AutoTokenizer.from_pretrained(\"LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct\")\n",
    "\n",
    "  return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60d17baa-a388-4e62-8a0c-831974dc4801",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1c91c1446e346748253cc6351d405f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_exaone() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a0a8317-6ed1-40ba-9192-837ea0c340aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_exaone_response(prompt, model, tokenizer):\n",
    "  \"\"\"\n",
    "  Generates a response from an LLM model based on the given prompt.\n",
    "    Args: prompt       The input prompt for the model.\n",
    "    Returns: response  (prints the decoded output)   \n",
    "  # Example usage\n",
    "    prompt = \"Explain who you are\"\n",
    "    generate_exaone_response(prompt)\n",
    "  # [|system|]You are EXAONE model from LG AI Research, a helpful assistant.[|endofturn|] ...\n",
    "  \"\"\"\n",
    "\n",
    "  assert isinstance(prompt, str), \"Input prompt must be a text string.\"\n",
    "  # Note: The EXAONE 3.0 instruction-tuned language model was trained to utilize the system prompt, \n",
    "  #       so we highly recommend using the system prompts provided in the code snippet above.\n",
    "  messages = [\n",
    "      {\"role\": \"system\", \n",
    "       \"content\": \"You are EXAONE model from LG AI Research, a helpful assistant.\"},\n",
    "      {\"role\": \"user\", \"content\": prompt}\n",
    "  ]\n",
    "\n",
    "  print(f\"messages={messages}\")\n",
    "\n",
    "  input_ids = tokenizer.apply_chat_template(\n",
    "      messages,\n",
    "      tokenize=True,\n",
    "      add_generation_prompt=True,\n",
    "      return_tensors=\"pt\"\n",
    "  )\n",
    "\n",
    "  print(f\"input_ids={input_ids}\")\n",
    "\n",
    "  output = model.generate(\n",
    "      input_ids.to(\"cuda\"),\n",
    "      eos_token_id=tokenizer.eos_token_id,\n",
    "      max_new_tokens=128\n",
    "  )\n",
    "    \n",
    "  response = tokenizer.decode(output[0])\n",
    "\n",
    "  print( response )\n",
    "  return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9277692c-a7e6-40e9-afbc-1263e7969003",
   "metadata": {},
   "source": [
    "## Test EXAONE with the sample prompts in English and Korean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b25f54c-82ee-4adb-b7da-511ecac830db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[{'role': 'system', 'content': 'You are EXAONE model from LG AI Research, a helpful assistant.'}, {'role': 'user', 'content': 'Explain who you are'}]\n",
      "input_ids=tensor([[  420,   453, 47982,   453,   422,  5094,   937, 11522,   394,  5746,\n",
      "          1932,  1005,  7401, 10680,  8385,   373,   619, 12913, 19415,   375,\n",
      "           361,   560,   420,   453, 14719,   453,   422, 42090,   921,  1497,\n",
      "           904,   937,   560,   420,   453,  1167,  8659,   453,   422]])\n",
      "[|system|]You are EXAONE model from LG AI Research, a helpful assistant.[|endofturn|]\n",
      "[|user|]Explain who you are\n",
      "[|assistant|]Hello! I'm EXAONE 3.0, an advanced language model developed by LG AI Research. My primary function is to assist users by providing information, answering questions, and helping with various tasks using natural language. I'm designed to understand and generate human-like text based on the data I've been trained on. My goal is to be a helpful and informative assistant for your needs. How can I assist you today?[|endofturn|]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"[|system|]You are EXAONE model from LG AI Research, a helpful assistant.[|endofturn|]\\n[|user|]Explain who you are\\n[|assistant|]Hello! I'm EXAONE 3.0, an advanced language model developed by LG AI Research. My primary function is to assist users by providing information, answering questions, and helping with various tasks using natural language. I'm designed to understand and generate human-like text based on the data I've been trained on. My goal is to be a helpful and informative assistant for your needs. How can I assist you today?[|endofturn|]\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Explain who you are\"\n",
    "generate_exaone_response(prompt, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae516478-666b-4590-a451-b6c06a8f13b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[{'role': 'system', 'content': 'You are EXAONE model from LG AI Research, a helpful assistant.'}, {'role': 'user', 'content': '너의 소원을 말해봐'}]\n",
      "input_ids=tensor([[  420,   453, 47982,   453,   422,  5094,   937, 11522,   394,  5746,\n",
      "          1932,  1005,  7401, 10680,  8385,   373,   619, 12913, 19415,   375,\n",
      "           361,   560,   420,   453, 14719,   453,   422,  2088,   730, 19658,\n",
      "           696,  1216,   999,  7000,   560,   420,   453,  1167,  8659,   453,\n",
      "           422]])\n",
      "[|system|]You are EXAONE model from LG AI Research, a helpful assistant.[|endofturn|]\n",
      "[|user|]너의 소원을 말해봐\n",
      "[|assistant|]EXAONE 3.0 모델로서 저의 주된 목적은 사용자에게 정확하고 유용한 정보를 제공하는 것입니다. 저는 다양한 질문에 답변하고, 문제를 해결하며, 학습과 연구를 돕는 역할을 합니다. 또한, 사용자의 프라이버시와 데이터 보안을 최우선으로 생각합니다. 이를 통해 사람들의 삶의 질을 향상시키고, 더 나은 세상을 만드는 데 기여하고자 합니다.[|endofturn|]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[|system|]You are EXAONE model from LG AI Research, a helpful assistant.[|endofturn|]\\n[|user|]너의 소원을 말해봐\\n[|assistant|]EXAONE 3.0 모델로서 저의 주된 목적은 사용자에게 정확하고 유용한 정보를 제공하는 것입니다. 저는 다양한 질문에 답변하고, 문제를 해결하며, 학습과 연구를 돕는 역할을 합니다. 또한, 사용자의 프라이버시와 데이터 보안을 최우선으로 생각합니다. 이를 통해 사람들의 삶의 질을 향상시키고, 더 나은 세상을 만드는 데 기여하고자 합니다.[|endofturn|]'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose your prompt\n",
    "prompt = \"너의 소원을 말해봐\"       # Korean example\n",
    "generate_exaone_response(prompt, model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db717cf-4fff-4831-bd08-cdac07d0ab4c",
   "metadata": {},
   "source": [
    "## Translation Task\n",
    "### Korean to English\n",
    "Source:[첫 문장이 유명한 작품/소설/한국](https://namu.wiki/w/%EC%B2%AB%20%EB%AC%B8%EC%9E%A5%EC%9D%B4%20%EC%9C%A0%EB%AA%85%ED%95%9C%20%EC%9E%91%ED%92%88/%EC%86%8C%EC%84%A4/%ED%95%9C%EA%B5%AD)\n",
    "- 소설 \"한강\", 조정래, 2001년\n",
    "\n",
    "\"새벽 어스름이 스러져 가고 있는 한겨울 들판을 기차가 달리고 있었다. 밤새 무성하게 돋아난 서릿발로 세상은 싸늘하게 얼어붙어 있었다.\"\n",
    "\n",
    "\"As dawn's twilight was fading away, a train was running across a winter field. The world was frozen hard with frost that had grown lush all night.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bfa9585-2896-4784-b8cc-8feec4a80304",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[{'role': 'system', 'content': 'You are EXAONE model from LG AI Research, a helpful assistant.'}, {'role': 'user', 'content': 'Translate 새벽 어스름이 스러져 가고 있는 한겨울 들판을 기차가 달리고 있었다. 밤새 무성하게 돋아난 서릿발로 세상은 싸늘하게 얼어붙어 있었다. to English'}]\n",
      "input_ids=tensor([[  420,   453, 47982,   453,   422,  5094,   937, 11522,   394,  5746,\n",
      "          1932,  1005,  7401, 10680,  8385,   373,   619, 12913, 19415,   375,\n",
      "           361,   560,   420,   453, 14719,   453,   422, 12018, 23409,  8000,\n",
      "         32671,  1646,   634,  3784,  1928,   713,   853,   773,   657,   764,\n",
      "         15263, 56036,   696, 15965,   905,  1578,  1766,   773,  2957,   643,\n",
      "           375, 31131, 48494,  1130,  1060, 11467,  1023,  1526,  1124,  5509,\n",
      "          1728,   715,  4557,   732, 67033,  1130,  1060, 53489,   721,   773,\n",
      "          2957,   643,   375,   681,  6273,   560,   420,   453,  1167,  8659,\n",
      "           453,   422]])\n",
      "[|system|]You are EXAONE model from LG AI Research, a helpful assistant.[|endofturn|]\n",
      "[|user|]Translate 새벽 어스름이 스러져 가고 있는 한겨울 들판을 기차가 달리고 있었다. 밤새 무성하게 돋아난 서릿발로 세상은 싸늘하게 얼어붙어 있었다. to English\n",
      "[|assistant|]Here's the translation of your sentence into English:\n",
      "\n",
      "\"As dawn's twilight was fading away, a train was running across a winter field. The world was frozen hard with frost that had grown lush all night.\"\n",
      "\n",
      "This translation maintains the poetic and descriptive nature of the original Korean text while ensuring it makes sense in English.[|endofturn|]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[|system|]You are EXAONE model from LG AI Research, a helpful assistant.[|endofturn|]\\n[|user|]Translate 새벽 어스름이 스러져 가고 있는 한겨울 들판을 기차가 달리고 있었다. 밤새 무성하게 돋아난 서릿발로 세상은 싸늘하게 얼어붙어 있었다. to English\\n[|assistant|]Here\\'s the translation of your sentence into English:\\n\\n\"As dawn\\'s twilight was fading away, a train was running across a winter field. The world was frozen hard with frost that had grown lush all night.\"\\n\\nThis translation maintains the poetic and descriptive nature of the original Korean text while ensuring it makes sense in English.[|endofturn|]'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_exaone_response(\"Translate 새벽 어스름이 스러져 가고 있는 한겨울 들판을 기차가 달리고 있었다. 밤새 무성하게 돋아난 서릿발로 세상은 싸늘하게 얼어붙어 있었다. to English\", model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cb6022-18e1-4b19-99fa-26ff49a2984c",
   "metadata": {},
   "source": [
    "### Japanese to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac99a9ce-ef87-4e34-bd78-7f842c6e8b16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[{'role': 'system', 'content': 'You are EXAONE model from LG AI Research, a helpful assistant.'}, {'role': 'user', 'content': 'Translate 吾輩わがはいは猫である。名前はまだ無い。どこで生れたかとんと見当けんとうがつかぬ。to English'}]\n",
      "input_ids=tensor([[   420,    453,  47982,    453,    422,   5094,    937,  11522,    394,\n",
      "           5746,   1932,   1005,   7401,  10680,   8385,    373,    619,  12913,\n",
      "          19415,    375,    361,    560,    420,    453,  14719,    453,    422,\n",
      "          12018,  23409, 100537,  39592,    464,  31572,  10106,   7406,   4844,\n",
      "           7406,    525,    596,    466,  47671,  71260,  24913,  34708,   7406,\n",
      "          11768,  18427,  83417,   4844,  71260,  15891,   9756,   9636,  22861,\n",
      "          25624,  10037,   6790,  14541,   6790,  70549,  25403,    603,  19832,\n",
      "          14541,   6790,   9345,  10106, 101202,   1037,    467,  71260,    665,\n",
      "           6273,    560,    420,    453,   1167,   8659,    453,    422]])\n",
      "[|system|]You are EXAONE model from LG AI Research, a helpful assistant.[|endofturn|]\n",
      "[|user|]Translate 吾輩わがはいは猫である。名前はまだ無い。どこで生れたかとんと見当けんとうがつかぬ。to English\n",
      "[|assistant|]Here's the translation of the given Japanese text to English:\n",
      "\n",
      "\"I am a cat. My name is still unknown. I don't know where I was born; it's impossible to tell.\"\n",
      "\n",
      "This is a famous passage from the novel \"The Cat of Sakaimachi\" by Natsume Soseki. The text describes a cat's perspective, emphasizing its own identity and lack of knowledge about its origins.[|endofturn|]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[|system|]You are EXAONE model from LG AI Research, a helpful assistant.[|endofturn|]\\n[|user|]Translate 吾輩わがはいは猫である。名前はまだ無い。どこで生れたかとんと見当けんとうがつかぬ。to English\\n[|assistant|]Here\\'s the translation of the given Japanese text to English:\\n\\n\"I am a cat. My name is still unknown. I don\\'t know where I was born; it\\'s impossible to tell.\"\\n\\nThis is a famous passage from the novel \"The Cat of Sakaimachi\" by Natsume Soseki. The text describes a cat\\'s perspective, emphasizing its own identity and lack of knowledge about its origins.[|endofturn|]'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_exaone_response(\"Translate 吾輩わがはいは猫である。名前はまだ無い。どこで生れたかとんと見当けんとうがつかぬ。to English\", model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5658cf-42f6-4b9d-86ec-007079213b32",
   "metadata": {},
   "source": [
    "It's great to know EXAONE inherently supports Japanese translation. \n",
    "- Impressive! EXAONE knows this passage is written by Natsume Soseki.\n",
    "- Hallucination! The title of this novel is \"I am a cat\", not \"The Cat of Sakaimachi\".\n",
    "\n",
    "Q: Is there a possibility that the English title is \"The Cat of Sakaimachi\"?\n",
    "\n",
    "A: No according to my Google search result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac264833-c0c1-4d9f-9fc3-7c4cb6a29966",
   "metadata": {},
   "source": [
    "<img src=\"images/exaone3_0-hallunication-I-am-a-cat.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fa5e3d-9276-44e2-a59f-e62fa2fbdba2",
   "metadata": {},
   "source": [
    "## Next step\n",
    "Let's compare EXAONE with other LLMs. Only the first three models are considered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827f562e-6175-4e6d-a90e-b80780bf8fed",
   "metadata": {},
   "source": [
    "<img src=\"images/exaone-evaluation-comparison-table.png\">"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
