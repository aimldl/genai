{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c972b40-599a-4846-8d79-19097ac22ebf",
   "metadata": {},
   "source": [
    "# Getting Started with EXAONE 3.0\n",
    "- Created: 2024-12-02 (Mon)\n",
    "- Updated: 2024-12-02 (Mon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1e7092-7906-488d-9b07-d2e833b85770",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "This guide provides a prcatical guide for utilizing the EXAONE language model. While the model is available via the Hugging Face repository (https://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct), implementing it requires some initial setup. This document offers a clear and concise walkthrough to facilitate a smooth and efficient integration of EXAONE into your projects. This notebook will guide you through the necessary steps to execute the code snippet successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaec9a9-eaa4-49ca-bddf-93e8b9f695bc",
   "metadata": {},
   "source": [
    "```python\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct\")\n",
    "\n",
    "# Choose your prompt\n",
    "prompt = \"Explain who you are\"  # English example\n",
    "prompt = \"너의 소원을 말해봐\"   # Korean example\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \n",
    "     \"content\": \"You are EXAONE model from LG AI Research, a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids.to(\"cuda\"),\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=128\n",
    ")\n",
    "print(tokenizer.decode(output[0]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329e3deb-8bcb-4339-bb63-5934dd62e85b",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "Make sure your instance meets both CPU and GPU requirements.\n",
    "- Vertex AI Workbench instance's default settings or `n1-standard-1` will fail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7181c3b6-fb1b-499a-825d-eb39d50a866c",
   "metadata": {},
   "source": [
    "<img src=\"images/vertex-ai-workbench-instance-details-hardware-default-options-without-nvidia-t4-gpu.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af467d4-dd87-4bf2-b7fb-ecd34a06a262",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9278c70-62d3-4fb7-a8af-4686a591ea5a",
   "metadata": {},
   "source": [
    "- Adding NVIDIA T4 GPU will also fail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fbfe33-f963-4ef6-868e-f3e1bb8ca951",
   "metadata": {},
   "source": [
    "<img src=\"images/vertex-ai-workbench-instance-t4-is-default.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2f8546-4eba-470d-8efd-5258ba89d5ad",
   "metadata": {},
   "source": [
    "That is,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac1c8ce-5033-4aa0-8567-f87da09b7b5a",
   "metadata": {},
   "source": [
    "<img src=\"images/vertex-ai-workbench-instance-details-hardware-default-options.png\" width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66427637-6505-485d-8580-7645a0fd978c",
   "metadata": {
    "tags": []
   },
   "source": [
    "An instance with NVIDIA L4 GPU or above is required to run EXAONE successfully.\n",
    "\n",
    "| Machine type  | CPU     | CPU RAM | Enough? | GPU    | GPU RAM | Enough? | Memo                                 |\n",
    "|---------------|---------|---------|---------|--------|---------|---------|--------------------------------------|\n",
    "| n1-standard-1 | 1 vCPUs | 3.75 GB | N       | T4 x 1 | 16 GB   | N/A     | RuntimeError                         |\n",
    "| n1-standard-2 | 2 vCPUs | 7.5 GB  | Y       | T4 x 1 | 16 GB   | N       | OutOfMemoryError: CUDA out of memory |\n",
    "| g2-standard-4 | 4 vCPUs | 16 GB   | Y       | L4 x 1 | 24 GB   | Y       | (No error)                           |\n",
    "\n",
    "CPU RAM size:\n",
    "- 3.75 GB is insufficient.\n",
    "- 7.5  GB is good enough.\n",
    "\n",
    "GPU RAM size:\n",
    "- 16 GB on T4 is insufficient.\n",
    "- 24 GB on L4 is good enough.\n",
    "\n",
    "Note that [NVIDIA L4 GPU](https://www.nvidia.com/en-us/data-center/l4/) has \n",
    "- 1.5x more GPU RAM and\n",
    "- 2.5x more Gen AI performance\n",
    "\n",
    "than [NVIDIA T4 GPU](https://www.nvidia.com/en-us/data-center/tesla-t4/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9eeeff-0cf1-4c4a-9540-c2f869f214b3",
   "metadata": {},
   "source": [
    "## Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a917f45-f7a4-4539-9dc8-ce621b2ab7db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage:   \n",
      "  pip install [options] <requirement specifier> [package-index-options] ...\n",
      "  pip install [options] -r <requirements file> [package-index-options] ...\n",
      "  pip install [options] [-e] <vcs project url> ...\n",
      "  pip install [options] [-e] <local project path> ...\n",
      "  pip install [options] <archive url/path> ...\n",
      "\n",
      "no such option: --quite\n"
     ]
    }
   ],
   "source": [
    "# ModuleNotFoundError: No module named 'torch'\n",
    "!pip install --upgrade --quiet torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4025bf5-05e7-4a55-9903-da50fa2dba59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# ModuleNotFoundError: No module named 'transformers'\n",
    "!pip install --quiet transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "257f4335-a799-4614-b75a-83c2374b8c68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.26.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "# OSError: You are trying to access a gated repo. Make sure to have access to it at https:// ...\n",
    "!pip install --quiet huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970f07ad-7a83-4eca-922a-ce0585eaba29",
   "metadata": {},
   "source": [
    "## Log into Hugging Face Hub\n",
    "The Hugging Face token is created and saved in `token-to-access-exaone-since-2024-q4.txt`.\n",
    "\n",
    "Open a new terminal (File > New > Terminal) and run:\n",
    "```bash\n",
    "$ huggingface-cli login\n",
    "# Paste your access token when prompted.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b8ee00f-2174-42bc-9d59-450983a8547b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ImportError: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install 'accelerate>=0.26.0'`\n",
    "!pip install accelerate>=0.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48e97dac-4116-4b69-b1d2-1cedda93e75c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.1\n"
     ]
    }
   ],
   "source": [
    "# Double-check\n",
    "import accelerate\n",
    "print(accelerate.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4280895f-860b-4624-98cd-60b7a463cabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart runtime\n",
    "#   Otherwise, the ImportError won't go away.\n",
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6363f3-7883-43f6-bca0-4576d29285e6",
   "metadata": {},
   "source": [
    "A significant amount of RAM is required to load the EXAONE-3.0-7.8B-Instruct model.\n",
    "If RAM is insufficient, a `RuntimeError` is expected. \n",
    "```bash\n",
    "# RuntimeError: unable to mmap 4932636680 bytes from file </home/jupyter/.cache/huggingface/hub/models--LGAI-EXAONE--EXAONE-3.0-7.8B-Instruct/snapshots/7f15baedd46858153d817445aff032f4d6cf4939/model-00001-of-00007.safetensors>: Cannot allocate memory (12)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d2f6d0f-ba50-457b-b31f-865501cbe651",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               total        used        free      shared  buff/cache   available\n",
      "Mem:            15Gi       1.0Gi        13Gi       0.0Ki       1.2Gi        14Gi\n",
      "Swap:             0B          0B          0B\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed178fe-6628-4bff-8207-7be8b84a0c38",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    " (,  RAM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbb99b44-1918-443d-845e-d593e6a069b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b5f3ba1-42fa-45b3-9249-555c5319d591",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-02 05:00:34\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ad056f3-f570-4700-9f30-648594e746fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0d45c9f0c4348b28d963b15e3cf0de4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-02 05:03:02\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3f3f81e-3227-4cfa-b403-5de5ad75db04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-02 05:03:02\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct\")\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce826f85-fa21-4aa5-bf83-f9aa69c853cd",
   "metadata": {},
   "source": [
    "## Find the sufficiently large instance type\n",
    "\n",
    "### OutOfMemoryError: CUDA out of memory\n",
    "```bash\n",
    "OutOfMemoryError: CUDA out of memory. Tried to allocate 800.00 MiB. GPU 0 has a total capacity of 14.57 GiB of which 708.75 MiB is free. Including non-PyTorch memory, this process has 13.87 GiB memory in use. Of the allocated memory 12.87 GiB is allocated by PyTorch, and 898.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
    "```\n",
    "\n",
    "### Use NVIDIA L4 GPU instead of T4\n",
    "| Machine type  | CPU     | RAM     | Enough? | GPU    | GPU RAM | Enough? | Memo                                 |\n",
    "|---------------|---------|---------|---------|--------|---------|---------|--------------------------------------|\n",
    "| n1-standard-1 | 1 vCPUs | 3.75 GB | N       | T4 x 1 | 16 GB   | N/A     | RuntimeError                         |\n",
    "| n1-standard-2 | 2 vCPUs | 7.5 GB  | Y       | T4 x 1 | 16 GB   | N       | OutOfMemoryError: CUDA out of memory |\n",
    "| g2-standard-4 | 4 vCPUs | 16 GB   | Y       | L4 x 1 | 24 GB   | Y       | (No error)                           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a53d5f3-56f4-4c4e-b1e2-a5e8fb713f17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23034 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# See the total GPU memory\n",
    "!nvidia-smi --query-gpu=memory.total --format=csv,noheader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ecac20f5-09c5-4f2c-a211-87cc7b2f3402",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15479, 7001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# See \"used memory, free memory\"\n",
    "!nvidia-smi --query-gpu=memory.used,memory.free --format=csv,noheader,nounits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e6af3391-8cfc-4fbe-abb9-5834a68a5ab4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec  2 06:34:57 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA L4                      On  |   00000000:00:03.0 Off |                    0 |\n",
      "| N/A   75C    P0             35W /   72W |   15479MiB /  23034MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      2852      C   python                                      15470MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# See more details,\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c49c5d0-63b7-471d-a88d-332c00176450",
   "metadata": {},
   "source": [
    "## Test EXAONE with the sample prompt in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29b0a6ec-ef15-48e6-8def-79d3f9ce9a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your prompt\n",
    "prompt = \"Explain who you are\"  # English example\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \n",
    "     \"content\": \"You are EXAONE model from LG AI Research, a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a9367c4-cf53-4edf-a562-5f3db5f20a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-02 05:17:55\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4d4ef8a-c341-4f40-b2fb-90bd1fa3ba90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-02 05:17:55\n",
      "2024-12-02 05:18:01\n",
      "[|system|]You are EXAONE model from LG AI Research, a helpful assistant.[|endofturn|]\n",
      "[|user|]Explain who you are\n",
      "[|assistant|]Hello! I'm EXAONE 3.0, an advanced language model developed by LG AI Research. My primary function is to assist users by providing information, answering questions, and helping with various tasks using natural language. I'm designed to understand and generate human-like text based on the data I've been trained on. My goal is to be a helpful and informative assistant for your needs. How can I assist you today?[|endofturn|]\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "output = model.generate(\n",
    "    input_ids.to(\"cuda\"),\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=128\n",
    ")\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9277692c-a7e6-40e9-afbc-1263e7969003",
   "metadata": {},
   "source": [
    "## Test EXAONE with the sample prompt in Korean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae516478-666b-4590-a451-b6c06a8f13b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose your prompt\n",
    "prompt = \"너의 소원을 말해봐\"       # Korean example\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \n",
    "     \"content\": \"You are EXAONE model from LG AI Research, a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3578c3e1-cf9d-4003-a726-00bdda174e73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[|system|]You are EXAONE model from LG AI Research, a helpful assistant.[|endofturn|]\n",
      "[|user|]너의 소원을 말해봐\n",
      "[|assistant|]EXAONE 3.0 모델로서 저의 주된 목적은 사용자에게 정확하고 유용한 정보를 제공하는 것입니다. 저는 다양한 질문에 답변하고, 문제를 해결하며, 학습과 연구를 돕는 역할을 합니다. 또한, 사용자의 프라이버시와 데이터 보안을 최우선으로 생각합니다. 이를 통해 사람들의 삶의 질을 향상시키고, 더 나은 세상을 만드는 데 기여하고자 합니다.[|endofturn|]\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids.to(\"cuda\"),\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=128\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3873cc00-d21b-4851-babe-1d9f66f1f17a",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "You have successfully run EXAONE 3.0!"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
